
@inproceedings{amdahl_validitysingle_1967,
  abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
  author = {Amdahl, Gene M.},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, {{Spring Joint Computer Conference}}},
  date = {1967},
  doi = {10.1145/1465482.1465560},
  langid = {english},
  location = {{New York, NY, USA}},
  pages = {483--485},
  publisher = {{ACM Press}},
  series = {{{AFIPS}} '67 ({{Spring}})},
  title = {Validity of the {{Single Processor Approach}} to {{Achieving Large Scale Computing Capabilities}}},
  venue = {Atlantic City, New Jersey}
}

@article{arber_contemporaryparticleincell_2015,
  abstract = {Particle-in-cell (PIC) methods have a long history in the study of laser-plasma interactions. Early electromagnetic codes used the Yee staggered grid for field variables combined with a leapfrog EM-field update and the Boris algorithm for particle pushing. The general properties of such schemes are well documented. Modern PIC codes tend to add to these high-order shape functions for particles, Poisson preserving field updates, collisions, ionisation, a hybrid scheme for solid density and high-field QED effects. In addition to these physics packages, the increase in computing power now allows simulations with real mass ratios, full 3D dynamics and multi-speckle interaction. This paper presents a review of the core algorithms used in current laser-plasma specific PIC codes. Also reported are estimates of self-heating rates, convergence of collisional routines and test of ionisation models which are not readily available elsewhere. Having reviewed the status of PIC algorithms we present a summary of recent applications of such codes in laser-plasma physics, concentrating on SRS, short-pulse laser-solid interactions, fast-electron transport, and QED effects.},
  author = {Arber, T D and Bennett, K and Brady, C S and Lawrence-Douglas, A and Ramsay, M G and Sircombe, N J and Gillies, P and Evans, R G and Schmitz, H and Bell, A R and Ridgers, C P},
  date = {2015-11-01},
  doi = {10.1088/0741-3335/57/11/113001},
  issn = {0741-3335, 1361-6587},
  journaltitle = {Plasma Physics and Controlled Fusion},
  langid = {english},
  number = {11},
  pages = {1--26},
  title = {Contemporary Particle-in-Cell Approach to Laser-Plasma Modelling},
  volume = {57}
}

@book{arnold_mathematicalmethods_1989,
  abstract = {In this text, the author constructs the mathematical apparatus of classical mechanics from the beginning, examining all the basic problems in dynamics, including the theory of oscillations, the theory of rigid body motion, and the Hamiltonian formalism. This modern approch, based on the theory of the geometry of manifolds, distinguishes iteself from the traditional approach of standard textbooks. Geometrical considerations are emphasized throughout and include phase spaces and flows, vector fields, and Lie groups. The work includes a detailed discussion of qualitative methods of the theory of dynamical systems and of asymptotic methods like perturbation techniques, averaging, and adiabatic invariance.},
  author = {Arnol'd, V. I.},
  date = {1989},
  edition = {2},
  isbn = {978-0-387-96890-2},
  langid = {english},
  location = {{New York}},
  publisher = {{Springer-Verlag}},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  title = {Mathematical {{Methods}} of {{Classical Mechanics}}},
  url = {https://www.springer.com/gp/book/9780387968902},
  urldate = {2019-08-11}
}

@book{birdsall_plasmaphysics_2005,
  abstract = {PART 1: PRIMER Why attempting to do plasma physics via computer simulation using particles makes good sense Overall view of a one dimensional electrostatic program A one dimensional electrostatic program ES1 Introduction to the numerical methods used Projects for ES1 A 1d electromagnetic program EM1 Projects for EM1 PART 2: THEORY Effects of the spatial grid Effects of the finitw time ste Energy-conserving simulation models Multipole models Kinetic theory for fluctuations and noise; collisions Kinetic properties: theory, experience and heuristic estimates PART 3: PRACTIC.},
  author = {Birdsall, Charles K and Langdon, A. Bruce},
  date = {2005},
  isbn = {978-1-4822-6306-0},
  langid = {english},
  location = {{Bristol}},
  note = {OCLC: 968173834},
  publisher = {{Institute of Physics Pub.}},
  series = {Series in {{Plasma Physics}}},
  title = {Plasma Physics via Computer Simulation},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1675636},
  urldate = {2019-06-21}
}

@inproceedings{boris_relativisticplasma_1970,
  author = {Boris, Jay P.},
  booktitle = {Fourth {{Conference}} on {{Numerical Simulations}} of {{Plasmas}}},
  date = {1970-11-02},
  location = {{Naval Research Laboratory, Washington, D.C.}},
  pages = {3--67},
  publisher = {{Defense Technical Information Center}},
  title = {Relativistic {{Plasma Simulation}}--{{Optimization}} of a {{Hybrid Code}}},
  url = {http://archive.org/details/DTIC_ADA023511},
  urldate = {2019-08-07}
}

@article{brady_synchrotronradiation_2014,
  author = {Brady, C. S. and Ridgers, C. P. and Arber, T. D. and Bell, A. R.},
  date = {2014-03-01},
  doi = {10.1063/1.4869245},
  issn = {1070-664X},
  journaltitle = {Physics of Plasmas},
  number = {3},
  pages = {033108},
  shortjournal = {Physics of Plasmas},
  title = {Synchrotron Radiation, Pair Production, and Longitudinal Electron Motion during 10-100 {{PW}} Laser Solid Interactions},
  volume = {21}
}

@article{buneman_timereversibledifference_1967,
  author = {Buneman, O},
  date = {1967-06},
  doi = {10.1016/0021-9991(67)90056-3},
  issn = {00219991},
  journaltitle = {Journal of Computational Physics},
  langid = {english},
  number = {4},
  pages = {517-535},
  title = {Time-Reversible Difference Procedures},
  volume = {1}
}

@book{butcher_numericalmethods_2016,
  author = {Butcher, J. C.},
  date = {2016},
  edition = {Third edition},
  isbn = {978-1-119-12150-3},
  keywords = {Differential equations,Numerical solutions},
  langid = {english},
  location = {{Chichester, West Sussex, United Kingdom}},
  pagetotal = {513},
  publisher = {{Wiley}},
  title = {Numerical Methods for Ordinary Differential Equations}
}

@book{eisenberg_nucleartheory_1978,
  author = {Eisenberg, Judah M. and Greiner, Walter},
  date = {1978},
  edition = {2., rev. ed., 1. repr},
  isbn = {978-0-7204-0483-8},
  langid = {english},
  location = {{Amsterdam}},
  note = {OCLC: 256795525},
  pagetotal = {421},
  publisher = {{North-Holland [u.a.]}},
  shorttitle = {Nuclear Theory. 2},
  title = {Nuclear Theory. 2: {{Excitation}} Mechanisms of the Nucleus}
}

@article{fonseca_exploitingmultiscale_2013,
  abstract = {A new generation of laser wakefield accelerators (LWFA), supported by the extreme accelerating fields generated in the interaction of PW-Class lasers and underdense targets, promises the production of high quality electron beams in short distances for multiple applications. Achieving this goal will rely heavily on numerical modelling to further understand the underlying physics and identify optimal regimes, but large scale modelling of these scenarios is computationally heavy and requires the efficient use of state-of-the-art petascale supercomputing systems. We discuss the main difficulties involved in running these simulations and the new developments implemented in the OSIRIS framework to address these issues, ranging from multi-dimensional dynamic load balancing and hybrid distributed/shared memory parallelism to the vectorization of the PIC algorithm. We present the results of the OASCR Joule Metric program on the issue of large scale modelling of LWFA, demonstrating speedups of over 1 order of magnitude on the same hardware. Finally, scalability to over ∼106 cores and sustained performance over ∼2 P Flops is demonstrated, opening the way for large scale modelling of LWFA scenarios.},
  author = {Fonseca, R. A. and Vieira, J. and Fiuza, F. and Davidson, A. and Tsung, F. S. and Mori, W. B. and Silva, L. O.},
  date = {2013-11},
  doi = {10.1088/0741-3335/55/12/124011},
  issn = {0741-3335},
  journaltitle = {Plasma Physics and Controlled Fusion},
  langid = {english},
  number = {12},
  pages = {124011},
  shortjournal = {Plasma Phys. Control. Fusion},
  title = {Exploiting Multi-Scale Parallelism for Large Scale Numerical Modelling of Laser Wakefield Accelerators},
  volume = {55}
}

@inproceedings{fonseca_osiristhreedimensional_2002,
  abstract = {We describe OSIRIS, a three-dimensional, relativistic, massively parallel, object oriented particle-in-cell code for modeling plasma based accelerators. Developed in Fortran 90, the code runs on multiple platforms (Cray T3E, IBM SP, Mac clusters) and can be easily ported to new ones. Details on the code’s capabilities are given. We discuss the object-oriented design of the code, the encapsulation of system dependent code and the parallelization of the algorithms involved. We also discuss the implementation of communications as a boundary condition problem and other key characteristics of the code, such as the moving window, open-space and thermal bath boundaries, arbitrary domain decomposition, 2D (cartesian and cylindric) and 3D simulation modes, electron sub-cycling, energy conservation and particle and field diagnostics. Finally results from three-dimensional simulations of particle and laser wakefield accelerators are presented, in connection with the data analysis and visualization infrastructure developed to post-process the scalar and vector results from PIC simulations.},
  author = {Fonseca, R. A. and Silva, L. O. and Tsung, F. S. and Decyk, V. K. and Lu, W. and Ren, C. and Mori, W. B. and Deng, S. and Lee, S. and Katsouleas, T. and Adam, J. C.},
  booktitle = {Computational {{Science}} — {{ICCS}} 2002},
  date = {2002},
  editor = {Sloot, Peter M. A. and Hoekstra, Alfons G. and Tan, C. J. Kenneth and Dongarra, Jack J.},
  isbn = {978-3-540-47789-1},
  keywords = {Beam Plasma Interaction,Boundary Condition Problem,Electron Plasma Wave,Relativistic Particle,Weibel Instability},
  langid = {english},
  pages = {342-351},
  publisher = {{Springer Berlin Heidelberg}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  shorttitle = {{{OSIRIS}}},
  title = {{{OSIRIS}}: {{A Three}}-{{Dimensional}}, {{Fully Relativistic Particle}} in {{Cell Code}} for {{Modeling Plasma Based Accelerators}}}
}

@article{gales_introduction_2016,
  author = {Gales, S},
  date = {2016},
  issue = {Supplement},
  journaltitle = {Romanian Reports in Physics},
  langid = {english},
  pages = {S5--S10},
  title = {Introduction},
  url = {http://www.rrp.infim.ro/2016_68_S/S5.pdf},
  urldate = {2019-08-29},
  volume = {68}
}

@article{golomb_proofwords_1985,
  author = {Golomb, Solomon W.},
  date = {1985-03-01},
  doi = {10.2307/2689900},
  eprint = {10.2307/2689900?origin=crossref},
  eprinttype = {jstor},
  issn = {0025570X},
  journaltitle = {Mathematics Magazine},
  langid = {english},
  number = {2},
  pages = {107},
  shorttitle = {Proof without {{Words}}},
  title = {Proof without {{Words}}: {{A}} 2 × 2 {{Determinant Is}} the {{Area}} of a {{Parallelogram}}},
  volume = {58}
}

@article{gustafson_reevaluatingamdahl_1988,
  author = {Gustafson, John L.},
  date = {1988-05},
  doi = {10.1145/42411.42415},
  issn = {0001-0782},
  journaltitle = {Commun. ACM},
  note = {Citation Key Alias: gustafson\_reevaluatingamdahl\_1988a},
  number = {5},
  pages = {532--533},
  title = {Reevaluating {{Amdahl}}'s {{Law}}},
  volume = {31}
}

@article{hairer_energybehaviour_2018,
  abstract = {The Boris algorithm is a widely used numerical integrator for the motion of particles in a magnetic ﬁeld. This article proves near-conservation of energy over very long times in the special cases where the magnetic ﬁeld is constant or the electric potential is quadratic. When none of these assumptions is satisﬁed, it is illustrated by numerical examples that the numerical energy can have a linear drift or its error can behave like a random walk. If the system has a rotational symmetry and the magnetic ﬁeld is constant, then also the momentum is approximately preserved over very long times, but in a spatially varying magnetic ﬁeld this is generally not satisﬁed.},
  author = {Hairer, Ernst and Lubich, Christian},
  date = {2018-12},
  doi = {10.1007/s10543-018-0713-1},
  issn = {0006-3835, 1572-9125},
  journaltitle = {BIT Numerical Mathematics},
  langid = {english},
  number = {4},
  pages = {969-979},
  title = {Energy Behaviour of the {{Boris}} Method for Charged-Particle Dynamics},
  volume = {58}
}

@book{hairer_geometricnumerical_2006,
  author = {Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  date = {2006},
  edition = {2nd ed},
  isbn = {978-3-540-30663-4},
  keywords = {Differential equations,Hamiltonian systems,Numerical integration,Numerical solutions},
  langid = {english},
  location = {{Berlin; New York}},
  note = {OCLC: ocm69223213},
  number = {31},
  pagetotal = {644},
  publisher = {{Springer}},
  series = {Springer Series in Computational Mathematics},
  shorttitle = {Geometric Numerical Integration},
  title = {Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations}
}

@article{higuera_structurepreservingsecondorder_2017,
  author = {Higuera, A. V. and Cary, J. R.},
  date = {2017-04-24},
  doi = {10.1063/1.4979989},
  issn = {1070-664X},
  journaltitle = {Physics of Plasmas},
  langid = {english},
  number = {5},
  pages = {052104},
  shortjournal = {Physics of Plasmas},
  title = {Structure-Preserving Second-Order Integration of Relativistic Charged Particle Trajectories in Electromagnetic Fields},
  volume = {24}
}

@book{hockney_computersimulation_1988,
  author = {Hockney, Roger W. and Eastwood, James W.},
  date = {1988},
  edition = {Special student ed},
  isbn = {978-0-85274-392-8},
  keywords = {Computer simulation,Digital computer simulation,Particles},
  location = {{Bristol [England]; Philadelphia}},
  pagetotal = {540},
  publisher = {{A. Hilger}},
  title = {Computer Simulation Using Particles}
}

@thesis{huebl_injectioncontrol_2014,
  abstract = {Laser-driven plasma wakefield accelerators provide accelerating electric fields orders of magnitude higher compared to conventional accelerators. Towards the generation of quasi-monoenergetic, multi-gigaelectronvolt electron beams, a precise control in the femtosecond time scale of the injection of electrons is needed. In this diploma thesis a new computational method to study external injection of electrons in laser-wakefield accelerators was derived. By loading a relativistic, charged particle bunch with arbitrary distribution in energy, space and time new ways to study the properties of wakefield accelerated electrons are possible. Furthermore, the proposed scheme was implemented together with an advanced field solver to suppress numerical Cherenkov noise in the open source Particle-in-Cell code PIConGPU as they are critical to reduce numerical uncertainties in relativistic simulations. Powered with modern compute hardware (GPUs) it is now possible to reach a new quality of predictive simulations, running repeated simulations in a few hours compared to weeks as with today's legacy codes. New parallel algorithms to study the evolution of the acceleration process have been implemented such as the in-situ calculation of a two-dimensional phase space distribution. Providing live feedback from simulations introduces a paradigm change towards interactive numerical studies and dramatically reduces the amount of data for post-processing. Finally, numerical studies have been carried out benefiting from the new methods and implementations such as an extended down-ramp triggered self-injection scenario suitable for the reproducible generation of tunable electron bunches.},
  author = {Huebl, Axel},
  date = {2014-08-25},
  doi = {10.5281/zenodo.15924},
  keywords = {GPGPU,HPC,LPA,LWFA,PIC,Plasma Physics},
  langid = {english},
  title = {Injection {{Control}} for {{Electrons}} in {{Laser}}-{{Driven Plasma Wakes}} on the {{Femtosecond Time Scale}}},
  url = {https://zenodo.org/record/15924},
  urldate = {2019-06-25}
}

@online{ibm_power9servers_2018,
  author = {{IBM}},
  date = {2018},
  title = {{{POWER9 Servers Overview}}},
  url = {https://www.ibm.com/downloads/cas/KDQRVQRR},
  urldate = {2019-08-26}
}

@online{intelcorporation_intelxeon_2016,
  abstract = {Intel® Xeon Phi™ Processor 7290F (16GB, 1.50 GHz, 72 core) quick reference guide including specifications, features, pricing, compatibility, design documentation, ordering codes, spec codes and more.},
  author = {{Intel Corporation}},
  date = {2016},
  langid = {english},
  title = {{{Intel}}® {{Xeon Phi}}™ {{Processor 7290F}} ({{16GB}}, 1.50 {{GHz}}, 72 Core) {{Product Specifications}}},
  url = {https://ark.intel.com/content/www/us/en/ark/products/95831/intel-xeon-phi-processor-7290f-16gb-1-50-ghz-72-core.html},
  urldate = {2019-08-26}
}

@book{jackson_classicalelectrodynamics_1999,
  author = {Jackson, John David},
  date = {1999},
  edition = {3rd ed},
  isbn = {978-0-471-30932-1},
  keywords = {Electrodynamics},
  location = {{New York}},
  pagetotal = {808},
  publisher = {{Wiley}},
  title = {Classical Electrodynamics}
}

@article{kaneyee_numericalsolution_1966,
  author = {{Kane Yee}},
  date = {1966-05},
  doi = {10.1109/TAP.1966.1138693},
  issn = {0018-926X},
  journaltitle = {IEEE Transactions on Antennas and Propagation},
  langid = {english},
  number = {3},
  pages = {302-307},
  title = {Numerical Solution of Initial Boundary Value Problems Involving Maxwell's Equations in Isotropic Media},
  volume = {14}
}

@unpublished{karsch_applicationshigh_2018,
  author = {Karsch, Stefan},
  date = {2018},
  howpublished = {Lecture Notes},
  location = {{Ludwig-Maximilians-University Munich}},
  title = {Applications of {{High Intensity Laser Pulse}}},
  type = {Lecture Notes},
  url = {https://www.physik.uni-muenchen.de/lehre/vorlesungen/sose_18/applications_of_high-intensity_laser-pulses/vorlesung/LaserMatter.pdf},
  urldate = {2019-06-09}
}

@unpublished{lehe_electromagneticparticleincell_2018,
  author = {Lehe, Remi},
  eventtitle = {{{USPAS}} 2018: {{Simulation}} of {{Beam}} and {{Plasma Systems}}},
  title = {Electromagnetic {{Particle}}-{{In}}-{{Cell}} Codes},
  type = {Lecture notes},
  url = {https://people.nscl.msu.edu/~lund/uspas/sbp_2018/lec_em_pic/A1a_EM_PIC.pdf},
  urldate = {2019-08-15},
  venue = {{Old Dominion University Hampton, VA}},
  year = {2018 15--26 January}
}

@unpublished{lehe_electromagneticwave_2018,
  author = {Lehe, Remi},
  eventtitle = {{{USPAS}} 2018: {{Simulation}} of {{Beam}} and {{Plasma Systems}}},
  title = {Electromagnetic Wave Propagation in {{Particle}}-{{In}}-{{Cell}} Codes},
  type = {Lecture notes},
  url = {https://people.nscl.msu.edu/~lund/uspas/sbp_2018/lec_em_pic/A1b_EM_Waves.pdf},
  urldate = {2019-08-15},
  venue = {{Old Dominion University Hampton, VA}},
  year = {2018 15--26 January}
}

@book{leimkuhler_simulatinghamiltonian_2004,
  author = {Leimkuhler, B. and Reich, Sebastian},
  date = {2004},
  isbn = {978-0-521-77290-7},
  keywords = {Hamiltonian systems},
  location = {{Cambridge, UK; New York}},
  number = {14},
  pagetotal = {379},
  publisher = {{Cambridge University Press}},
  series = {Cambridge Monographs on Applied and Computational Mathematics},
  title = {Simulating {{Hamiltonian}} Dynamics}
}

@online{lin_scalabilitystrong_2018,
  abstract = {High performance computing (HPC) clusters are able to solve big problems using a large number of processors. This is also known as parallel computing, where many processors work simultaneously to p…},
  author = {Lin, Xin},
  date = {2018-11-09T08:47:45+00:00},
  journaltitle = {PDC Blog},
  langid = {british},
  shorttitle = {Scalability},
  title = {Scalability: Strong and Weak Scaling},
  url = {https://www.kth.se/blogs/pdc/2018/11/scalability-strong-and-weak-scaling/},
  urldate = {2019-08-29}
}

@book{marsden_introductionmechanics_1999,
  author = {Marsden, Jerrold E. and Ratiu, Tudor S.},
  date = {1999},
  doi = {10.1007/978-0-387-21792-5},
  editorb = {Marsden, Jerrold E. and Sirovich, L. and Golubitsky, M. and Jäger, W.},
  editorbtype = {redactor},
  isbn = {978-1-4419-3143-6 978-0-387-21792-5},
  langid = {english},
  location = {{New York, NY}},
  publisher = {{Springer New York}},
  series = {Texts in {{Applied Mathematics}}},
  shorttitle = {Introduction to {{Mechanics}} and {{Symmetry}}},
  title = {Introduction to {{Mechanics}} and {{Symmetry}}: {{A Basic Exposition}} of {{Classical Mechanical Systems}}},
  url = {http://link.springer.com/10.1007/978-0-387-21792-5},
  urldate = {2019-08-10},
  volume = {17}
}

@collection{mourou_eliextreme_2011,
  date = {2011},
  editor = {Mourou, Gérard A. and Korn, Georg and Sandner, Wolfgang and Collier, John L.},
  location = {{Wolfshagener Str. 56 13187, Berlin, Germany}},
  pagetotal = {536},
  publisher = {{THOSS Media GmbH}},
  title = {{{ELI}} - {{Extreme Light Infrastructure WHITEBOOK}}}
}

@online{nvidiacorporation_nvidianvlink_2018,
  abstract = {NVLink and NVSwitch provide ultra-fast communication between CPU and GPUs, and between GPUs.},
  author = {{NVIDIA Corporation}},
  date = {2018},
  journaltitle = {NVIDIA},
  langid = {american},
  shorttitle = {{{NVIDIA NVLink Fabric}}},
  title = {{{NVIDIA NVLink Fabric}}: {{Advanced Multi}}-{{GPU Processing}}},
  url = {https://www.nvidia.com/en-us/data-center/nvlink/},
  urldate = {2019-08-26}
}

@online{nvidiacorporation_nvidiatesla_2018,
  author = {{NVIDIA Corporation}},
  date = {2018},
  title = {{{NVIDIA TESLA V100 GPU ACCELERATOR}}},
  url = {https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf},
  urldate = {2019-08-26}
}

@thesis{oneil_laserwakefield_2017,
  author = {O'Neil, Aaron},
  date = {2017-03-20},
  institution = {{Queen's University Belfast}},
  title = {Laser {{Wakefield Acceleratror Simulations Using EPOCH}}},
  type = {MSci Thesis},
  url = {https://pure.qub.ac.uk/portal/files/147720282/Aaron_ONeill_40007530.pdf},
  urldate = {2019-03-17}
}

@article{qin_whyboris_2013,
  author = {Qin, Hong and Zhang, Shuangxi and Xiao, Jianyuan and Liu, Jian and Sun, Yajuan and Tang, William M.},
  date = {2013-08-01},
  doi = {10.1063/1.4818428},
  issn = {1070-664X},
  journaltitle = {Physics of Plasmas},
  number = {8},
  pages = {084503},
  shortjournal = {Physics of Plasmas},
  title = {Why Is {{Boris}} Algorithm so Good?},
  volume = {20}
}

@online{sharcnet_measuringparallel_2016,
  author = {{SHARCNET}},
  date = {2016-03-01},
  journaltitle = {SHARCNET},
  title = {Measuring {{Parallel Scaling Performance}} - {{Documentation}}},
  url = {https://www.sharcnet.ca/help/index.php/Measuring_Parallel_Scaling_Performance#Calculating_Strong_Scaling_Efficiency},
  urldate = {2019-08-30}
}

@online{strohmaier_top500supercomputer_1993,
  author = {Strohmaier, Erich and Dongarra, Jack and Simon, Horst},
  date = {1993/2019},
  journaltitle = {TOP 500 The List.},
  title = {{{TOP500 Supercomputer Sites}}},
  url = {https://www.top500.org/},
  urldate = {2019-08-28}
}

@book{stuart_dynamicalsystems_1996,
  author = {Stuart, A. M. and Humphries, A. R.},
  date = {1996},
  isbn = {978-0-521-49672-8},
  keywords = {Differentiable dynamical systems,Numerical analysis},
  location = {{Cambridge; New York}},
  number = {2},
  pagetotal = {685},
  publisher = {{Cambridge University Press}},
  series = {Cambridge Monographs on Applied and Computational Mathematics},
  title = {Dynamical Systems and Numerical Analysis}
}

@article{tajima_laserelectron_1979,
  abstract = {An intense electromagnetic pulse can create a weak of plasma oscillations through the action of the nonlinear ponderomotive force. Electrons trapped in the wake can be accelerated to high energy. Existing glass lasers of power density 1018W/cm2 shone on plasmas of densities 1018 cm−3 can yield gigaelectronvolts of electron energy per centimeter of acceleration distance. This acceleration mechanism is demonstrated through computer simulation. Applications to accelerators and pulsers are examined.},
  author = {Tajima, T. and Dawson, J. M.},
  date = {1979-07-23},
  doi = {10.1103/PhysRevLett.43.267},
  journaltitle = {Physical Review Letters},
  keywords = {Computer simulation,density,Electron,Lasers,Nonlinear system,Plasma Active},
  number = {4},
  pages = {267-270},
  shortjournal = {Phys. Rev. Lett.},
  title = {Laser {{Electron Accelerator}}},
  volume = {43}
}

@article{turcu_highfield_2016,
  abstract = {ELI-NP facility will enable for the first time the use of two 10 PW laser beams for quantum electrodynamics (QED) experiments. The first beam will accelerate electrons to relativistic energies. The second beam will subject relativistic electrons to the strong electromagnetic field generating QED processes: intense gamma ray radiation and electron-positron pair formation. The laser beams will be focused to intensities above 1021 Wcm-2 and reaching 1022-1023 Wcm-2 for the first time. We propose to use this capability to investigate new physical phenomena at the interfaces of plasma, nuclear and particle physics at ELI-NP. This High Power Laser System Technical Design Report (HPLS-TDR2) presents the experimental area E6 at ELI-NP for investigating high field physics and quantum electrodynamics and the production of electron-positron-pairs and of energetic gamma-rays. The scientific community submitted 12 commissioning runs for E6 interaction chamber with two 10 PW laser beams and one proposal for the CETAL interaction chamber with 1 PW laser. The proposals are representative of the international high field physics community being written by 48 authors from 14 European and US organizations. The proposals are classified according to the science area investigated into: Radiation Reaction Physics: Classical and Quantum; Compton and Thomson Scattering Physics: Linear and NonLinear Regimes; QED in Vacuum; Atoms in Extreme Fields. Two pump-probe colliding 10 PW laser beams are proposed for the E6 interaction chamber. The focused pump laser beam accelerates the electrons to relativistic energies. The accelerated electron bunches interact with the very high electro-magnetic field of the focused probe laser beam. We propose two main types of experiments with: (a) gas targets in which the pump laser-beam is focused by a long focal length mirror and drives a wakefield in which the electron bunch is accelerated to multi-GeV energies and then exposed to the EM field of the probe laser which is tightly focused; (b) solid targets in which both the pump and probe laser beams are focused on the solid target, one accelerating the electrons in the solid and the other, delayed, providing the high electric field to which the relativistic electrons are subjected. We propose four main focusing configurations for the pump and probe laser beams, two for each type of target: counter-propagating 10 PW focused laser beams and the two 10 PW laser beams focused in the same direction. For solid targets we propose an additional configuration with plasma-mirror on the pump laser beam: the plasma mirror placed between the focusing mirror and target. It is proposed that the 10 PW laser beams will have polarization control and focus control by means of adaptive optics. Initially only one 10 PW may have polarization control and adaptive optics. In order to accommodate the two laser beams and diagnostics the proposed interaction chamber is quasi-octagonal with a diameter of 4.5 m. A large electron-spectrometer is proposed for multi-GeV electrons. Other diagnostics are requested for: gamma-rays, electrons and positrons, protons and ions, plasma characterization, transmitted and reflected laser beam. Targets will be provided by the ELI-NP Target Laboratory or purchased. The E6 experiments and diagnostics will benefit from the ELI-NP Electronics Laboratory, the Workshop and the Optics Laboratory. In order to ensure radiation-protection, a large beam-dump is planned for both multi-GeV electrons and multi-100 MeV protons.},
  author = {Turcu, I C E and Negoita, F and Jaroszynski, D A and Mckenna, P and Balascuta, S and Ursescu, D and Dancus, I and Cernaianu, M O and Tataru, M V and Ghenuche, P and Stutman, D and Boianu, A and Risca, M and Toma, M and Petcu, C and Acbas, G and Yoffe, S R and Noble, A and Ersfeld, B and Brunetti, E and Capdessus, R and Murphy, C and Ridgers, C P and Neely, D and Mangles, S P D and Gray, R J and Thomas, A G R and Kirk, J G and Ilderton, A and Marklund, M and Gordon, D F and Hafizi, B and Kaganovich, D and Palastro, J P and D’Humieres, E and Zepf, M and Sarri, G and Gies, H and Karbstein, F and Schreiber, J and Paulus, G G and Dromey, B and Harvey, C and Piazza, A Di and Keitel, C H and Kaluza, M C and Gales, S and Zamfir, N V},
  date = {2016},
  issue = {Supplement},
  journaltitle = {Romanian Reports in Physics},
  langid = {english},
  pages = {S145--S231},
  title = {High {{Field Physics}} and {{Qed Experiments}} at {{ELI}}-{{NP}}},
  url = {http://www.rrp.infim.ro/2016_68_S/S145.pdf},
  urldate = {2019-08-29},
  volume = {68}
}

@article{vay_warpxnew_2018,
  abstract = {Turning the current experimental plasma accelerator state-of-the-art from a promising technology into mainstream scientific tools depends critically on high-performance, high-fidelity modeling of complex processes that develop over a wide range of space and time scales. As part of the U.S. Department of Energy’s Exascale Computing Project, a team from Lawrence Berkeley National Laboratory, in collaboration with teams from SLAC National Accelerator Laboratory and Lawrence Livermore National Laboratory, is developing a new plasma accelerator simulation tool that will harness the power of future exascale supercomputers for high-performance modeling of plasma accelerators. We present the various components of the codes such as the new Particle-In-Cell Scalable Application Resource (PICSAR) and the redesigned adaptive mesh refinement library AMReX, which are combined with redesigned elements of the Warp code, in the new WarpX software. The code structure, status, early examples of applications and plans are discussed.},
  author = {Vay, J.-L. and Almgren, A. and Bell, J. and Ge, L. and Grote, D. P. and Hogan, M. and Kononenko, O. and Lehe, R. and Myers, A. and Ng, C. and Park, J. and Ryne, R. and Shapoval, O. and Thévenet, M. and Zhang, W.},
  date = {2018-11-12},
  doi = {10.1016/j.nima.2018.01.035},
  issn = {0168-9002},
  journaltitle = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  keywords = {Laser wakefield accelerator,Particle accelerators,Particle-in-cell,Plasma based accelerators,Plasma simulations,Relativistic plasmas},
  pages = {476-479},
  series = {3rd {{European Advanced Accelerator Concepts}} Workshop ({{EAAC2017}})},
  shortjournal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  shorttitle = {Warp-{{X}}},
  title = {Warp-{{X}}: {{A}} New Exascale Computing Platform for Beam–Plasma Simulations},
  volume = {909}
}

@report{vazhkudai_designdeployment_2018,
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  author = {Vazhkudai, S. S. and de Supinski, B. R. and Bland, A. S. and Geist, A. and Sexton, J. and Kahle, J. and Zimmer, C. J. and Atchley, S. and Oral, S. H. and Maxwell, D. E. and Vergara Larrea, V. G. and Bertsch, A. and Goldstone, R. and Joubert, W. and Chambreau, C. and Appelhans, D. and Blackmore, R. and Casses, B. and Chochia, G. and Davison, G. and Ezell, M. A. and Gonsiorowski, E. and Grinberg, L. and Hanson, B. and Hartner, B. and Karlin, I. and Leininger, M. L. and Leverman, D. and Marroquin, C. and Moody, A. and Ohmacht, M. and Pankajakshan, R. and Pizzano, F. and Rogers, J. H. and Rosenburg, B. and Schmidt, D. and Shankar, M. and Wang, F. and Watson, P. and Walkup, B. and Weems, L. D. and Yin, J.},
  date = {2018-07-30},
  institution = {{Lawrence Livermore National Lab. (LLNL), Livermore, CA (United States)}},
  langid = {english},
  number = {LLNL-CONF-755733},
  options = {useprefix=true},
  title = {The {{Design}}, {{Deployment}}, and {{Evaluation}} of the {{CORAL Pre}}-{{Exascale Systems}}},
  url = {https://www.osti.gov/biblio/1489443-design-deployment-evaluation-coral-pre-exascale-systems},
  urldate = {2019-08-26}
}

@article{vincenti_ultrahighordermaxwell_2018,
  abstract = {The advent of massively parallel supercomputers, with their distributed-memory technology using many processing units, has favored the development of highly-scalable local low-order solvers at the expense of harder-to-scale global very high-order spectral methods. Indeed, FFT-based methods, which were very popular on shared memory computers, have been largely replaced by finite-difference (FD) methods for the solution of many problems, including plasmas simulations with electromagnetic Particle-In-Cell methods. For some problems, such as the modeling of so-called “plasma mirrors” for the generation of high-energy particles and ultra-short radiations, we have shown that the inaccuracies of standard FD-based PIC methods prevent the modeling on present supercomputers at sufficient accuracy. We demonstrate here that a new method, based on the use of local FFTs, enables ultrahigh-order accuracy with unprecedented scalability, and thus for the first time the accurate modeling of plasma mirrors in 3D.},
  author = {Vincenti, Henri and Vay, Jean-Luc},
  date = {2018-07-01},
  doi = {10.1016/j.cpc.2018.03.018},
  issn = {0010-4655},
  journaltitle = {Computer Physics Communications},
  keywords = {Electromagnetic Particle-In-Cell method,Finite-difference time-domain solver,Massively parallel pseudo-spectral solvers,Pseudo-spectral analytical time domain solver,Relativistic plasma mirrors},
  pages = {22-29},
  shortjournal = {Computer Physics Communications},
  title = {Ultrahigh-Order {{Maxwell}} Solver with Extreme Scalability for Electromagnetic {{PIC}} Simulations of Plasmas},
  volume = {228}
}


